# CI5652 - Identical Machine Scheduling
Proyecto de Dise√±o de Algoritmos II (CI5652) con soluciones aproximadas para el problema de Identical Machine Scheduling. Trimestre Abr-Jul 2024. Universidad Sim√≥n Bol√≠var.

## üìù Integrantes
- Ana Shek (19-10096)
- Santiago Finamore (18-10125)
- Jeamhowards Montiel (19-10234)

# ü§î Descripci√≥n del problema
Dado un conjunto de `n` tareas y `m` m√°quinas id√©nticas, el objetivo es asignar cada tarea a una m√°quina y determinar la secuencia de tareas en cada m√°quina de manera que se minimice la tardanza total (the total tardiness). Cada tarea `j` tiene un tiempo de procesamiento `p_j` y una fecha de vencimiento `d_j`. La tardanza de una tarea se calcula como `max(0, C_j - d_j)`, donde `C_j` es el tiempo de finalizaci√≥n del trabajo `j`.

# üìã INFORME DEL PROYECTO - SEGUNDO CORTE
El programa est√° implementado en C++ y consta de los siguientes archivos para este segundo corte:

- `grasp.cpp`: Archivo principal del programa que contiene la implementaci√≥n de una soluci√≥n utilizando GRASP para el problema.

üìÇ En la carpeta `benchmarks` se encuentran los casos de pruebas de la primera corte del proyecto para medir y comparar el rendimiento de diferentes algoritmos para solucionar el problema descrito.

## Definici√≥n de una perturbaci√≥n e implementaci√≥n de una b√∫squeda local iterada (ILS).

### üîÑ ILS

La implementaci√≥n del algoritmo de B√∫squeda Local Iterada (ILS) recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como el n√∫mero m√°ximo de iteraciones y la fuerza de perturbaci√≥n inicial. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Luego, aplica el algoritmo `RNA` a esta soluci√≥n inicial para obtener una soluci√≥n mejorada `S`.
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).

2. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se realiza una perturbaci√≥n a la soluci√≥n actual `current_schedule` aplicando un n√∫mero `p` de movimientos aleatorios.
    - Despu√©s de la perturbaci√≥n, se aplica el algoritmo `RNA` para mejorar la soluci√≥n perturbada.

3. **Evaluaci√≥n de soluciones:** 
    - Si la soluci√≥n mejorada tiene una tardanza total (`total_tardiness`) menor que la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n y se restablece la fuerza de perturbaci√≥n `p` a su valor inicial.
    - Si no mejora, se incrementa un contador `i`.

4. **Ajuste de la perturbaci√≥n:** 
    - Si el contador `i` alcanza el l√≠mite `itermax`, se incrementa la fuerza de perturbaci√≥n `p`. Si `p` excede un valor m√°ximo `pmax`, se restablece a su valor inicial.

5. **Repetici√≥n:** 
    - Se repiten los pasos 2-4 hasta que se agoten las iteraciones.

El algoritmo `ILS` busca explorar el espacio de soluciones mediante la combinaci√≥n de perturbaciones y optimizaci√≥n local, ayudando a escapar de √≥ptimos locales y encontrar soluciones mejores.

## Definici√≥n de reglas para movimientos que han de ser tab√∫s e implementaci√≥n de una b√∫squeda tab√∫.

### üö´ Tabu Search

La implementaci√≥n del algoritmo de B√∫squeda Tab√∫ recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como el n√∫mero m√°ximo de iteraciones y la tenencia de la lista tab√∫. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).

2. **Lista Tab√∫:** 
    - Se inicializa una lista tab√∫ para almacenar los movimientos que est√°n prohibidos temporalmente.

3. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se identifica la m√°quina con mayor tardanza (`tardiest_machine`).
    - Se generan m√∫ltiples vecinos de la soluci√≥n actual mediante movimientos aleatorios en la m√°quina con mayor tardanza.
    - Se selecciona el mejor vecino que no est√© en la lista tab√∫ o que mejora la mejor soluci√≥n conocida.

4. **Actualizaci√≥n de la lista Tab√∫:** 
    - Si el mejor vecino no est√° en la lista tab√∫ o mejora la mejor soluci√≥n conocida, se actualiza la soluci√≥n actual y se a√±ade uno de los √≠ndices de los trabajos restantes de la maquina con m√°s tardiness a la lista tab√∫, esto para asegurar que no se extraigan varias veces elementos de la misma maquina, evitando as√≠ ciclos.
    - Si la lista tab√∫ excede su tama√±o m√°ximo (`tabu_tenure`), se elimina el movimiento m√°s antiguo.

5. **Actualizaci√≥n de la mejor soluci√≥n:** 
    - Si el vecino seleccionado mejora la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n.

6. **Repetici√≥n:** 
    - Se repiten los pasos 3-5 hasta que se agoten las iteraciones.

El algoritmo `Tabu Search` busca explorar el espacio de soluciones evitando ciclos y escapando de √≥ptimos locales mediante el uso de una lista tab√∫ que proh√≠be ciertos movimientos temporalmente.

## Definici√≥n de un proceso de enfriado progresivo e implementaci√≥n de un recocido simulado.

### ‚ùÑÔ∏è Simulated Annealing

La implementaci√≥n del algoritmo de Recocido Simulado (Simulated Annealing) recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como la temperatura inicial y el factor de reducci√≥n de temperatura. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).
    - Se inicializa la temperatura `t` con un valor inicial `t0`.

2. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se generan m√∫ltiples vecinos de la soluci√≥n actual mediante movimientos aleatorios.
    - Se calcula la diferencia de tardanza (`delta`) entre el vecino y la soluci√≥n actual.

3. **Criterio de aceptaci√≥n:** 
    - Si el vecino tiene una tardanza menor o igual, se acepta.
    - Si el vecino tiene una tardanza mayor, se acepta con una probabilidad `exp(-delta / t)`.

4. **Actualizaci√≥n de la mejor soluci√≥n:** 
    - Si la soluci√≥n actual mejorada tiene una tardanza menor que la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n.

5. **Reducci√≥n de la temperatura:** 
    - Despu√©s de un n√∫mero fijo de iteraciones, se reduce la temperatura multiplic√°ndola por un factor `t_step`.
    - Si la temperatura cae por debajo de un umbral `epsilon`, se restablece a su valor inicial `t0`.

6. **Repetici√≥n:** 
    - Se repiten los pasos 2-5 hasta que se agoten las iteraciones.

El algoritmo `Simulated Annealing` busca explorar el espacio de soluciones permitiendo peores soluciones con una probabilidad decreciente, lo que ayuda a escapar de √≥ptimos locales y encontrar mejores soluciones globales.

## üé≤ Definici√≥n de un m√©todo de construci√≥n para una RCL e implementaci√≥n de GRASP.

### üë®‚Äç‚öñÔ∏è M√©todo de construci√≥n para una RCL

Para la definici√≥n del RCL en este problema, se utiliz√≥ el enfoque heur√≠stico Modified Due Date (MDD) explicado en el primer corte del proyecto:

1. Sea `S` una soluci√≥n parcialmente construida.
2. Se tiene una lista de tareas no programadas `U` en la soluci√≥n `S`.
3. Para cada m√°quina `j`, dividir `U` en dos subconjuntos `U1j` y `U2j` para `j` = 1, 2, ..., `m`.
    > `U1j` contiene las tareas que no se pueden completar en su fecha de vencimiento en la m√°quina `j`.

    > `U2j` contiene las tareas que s√≠ se pueden completar antes de su fecha de vencimiento en la m√°quina `j`.
4. De `U1j` y `U2j`, encontrar los subconjuntos `Œ≥j` y `Œªj` que contienen las tareas con el tiempo de procesamiento m√≠nimo y la fecha de vencimiento m√≠nima, respectivamente. 
5. Seleccionar una tarea `gj` de `Œ≥j` o `Œªj` que minimice el valor de MDD en la m√°quina `j`. El valor de MDD en la m√°quina `j` de una tarea `i` est√° dada por `MDD(j, i) = max(Cj + pi, di)` 
    > `Cj` es la suma del procesamiento de tiempo de las tareas que ya han sido programados en la m√°quina `j`.

    > `pi` es el tiempo de procesamiento de la tarea `i` con su fecha de vencimiento `di`.
6. Luego, sea `C` el conjunto resultante de tener cada par `<g, l>`, donde cada una de estos pares representa que la tarea `g` es la tarea que produce el menor valor MDD en la m√°quina `l`).
7. Se puede definir el costo de la funci√≥n de un elemento `<g, l>` en `C` como `c(<g, l>) = MDD(g, l)`.
8. Tambi√©n se define `c_min = min{ c(<g, l>) | <g, l> ‚àà C}` y `c_max = max{ c(<g, l>) | <g, l> ‚àà C}`.
9. Entonces, el `RCL = { <g, l> ‚àà C | c(<g, l>) <= c_min + Œ±(c_max - c_min)}`

### üé∞ GRASP

La implementaci√≥n del algoritmo GRASP recibe la informaci√≥n de las `n` tareas, la cantidad de las `m` m√°quinas, el `alpha` para el RCL y `el m√°ximo n√∫mero de iteraciones`. A continuaci√≥n se describen los pasos de la implementaci√≥n: 

1. El algoritmo empieza a generar una soluci√≥n inicial aleatoria `S`. Por ahora, se tiene que `S` es la mejor soluci√≥n que se tiene para el problema.
2. Ahora, para cada iteraci√≥n, se construye una soluci√≥n voraz aleatoria `S'`, el cual: 

    2.1. Se considera las tareas que a√∫n no han sido programadas y se aplica el enfoque heur√≠stico para la Lista Restringida de Candidatos (RCL). 

    2.2. De esta lista RCL, se escoge aleatoriamente un elemento: `<g, l>`. 

    2.3. Luego, del elemento seleccionado `<g, l>`, se le asigna la tarea `g` a la m√°quina `l`. 

    2.4. Se elimina la tarea `g` de la lista de tareas sin programar en esta soluci√≥n parcial construida `S'`.

    2.5. Se repite los pasos 2.1 a 2.4 hasta que no queden tareas sin programar.
3. Se reemplaza `S` por `S'` si el retraso total o total tardiness de las tareas en la soluci√≥n `S` es mayor que el de `S'`, en caso contrario, no se hace nada.
4. Se repite el paso 2 y 3 hasta que se acaben las iteraciones.

## Definici√≥n de un fenotipo/genotipo, operadores de cruce y mutaci√≥n e implementaci√≥n de un algoritmo gen√©tico.

## üöÄ Uso

Para compilar y ejecutar el programa, se debe ejecutar los siguientes comandos en la terminal:

```bash
make
```

```bash
cd target
```

```bash
./PROY2 <path_to_benchmarks> <algorithm>
```

Donde `<path_to_benchmarks>` es la ruta a la carpeta que contiene los casos de prueba y `<algorithm>` es el n√∫mero del algoritmo a ejecutar:

1. B√∫squeda Local Iterada (ILS)
2. B√∫squeda Tab√∫ 
3. Reconocido Simulado
4. GRASP
5. Algoritmo Gen√©tico

## üìÑ Analisis de resultados

A continuaci√≥n se presenta el an√°lisis de los resultados obtenidos al ejecutar el programa con los casos de prueba en la carpeta `benchmarks` en una **laptop** con **procesador AMD Ryzen 5 5500U**, **disco SSD**, **8GB de memoria RAM** y **WSL 2 Ubuntu**. Se tomar√° en cuenta los valores de tardanza √≥ptima obtenidos del paper de referencia de donde se obtuvo el `benchmark` para comparar los resultados obtenidos con los algoritmos implementados de ambos cortes del proyecto.


### üìä M√©tricas Comparativas
Las m√©tricas claves en el an√°lisis incluyen:

- **Tardanza Total (Total Tardiness)**: La suma de las tardanzas de todas las tareas.
- **Diferencia con la Soluci√≥n √ìptima (Optimal Solution Difference)**: La diferencia entre la soluci√≥n obtenida y la soluci√≥n √≥ptima.
- **Tiempo en segundos (Time in seconds)**: El tiempo en segundos tomado por el algoritmo.

As√≠ mismo, se emplearon diferentes par√°metros para cada algoritmo implementado en este segundo corte:
**Par√°metros del Iterated Local Search (ILS)**:
 * max_iter Cantidad m√°xima de iteraciones para el algoritmo ILS
 * p0 La fuerza de perturbaci√≥n inicial.
 * pmax El multiplicador m√°ximo de fuerza de perturbaci√≥n.
 * rnamax El n√∫mero m√°ximo de iteraciones para el algoritmo de RNA dentro de ILS.
 * itermax El n√∫mero m√°ximo de iteraciones antes de aumentar la fuerza de la perturbaci√≥n.

> ILS1: max_iter = 1500, p0 = 10, pmax = 4, rnamax = 100, itermax = 100

> ILS2: max_iter = 1000, p0 = 3, pmax = 15, rnamax = 70, itermax = 150

**Par√°metros del Tabu Search (TS)**:
 * max_iter El n√∫mero m√°ximo de iteraciones para el algoritmo de b√∫squeda tab√∫.
 * max_grn_iter El n√∫mero m√°ximo de iteraciones para generar vecinos dentro de cada iteraci√≥n.
 * tabu_tenure El n√∫mero de iteraciones durante las cuales un movimiento permanece en la lista tab√∫.

> TS1: max_iter = 10000, max_grn_iter = 100, tabu_ternure = 7

> TS2: max_iter = 6000, max_grn_iter = 70, tabu_ternure = 5

**Par√°metros del Simulated Annealing (SA) o Reconocido Simulado**:
 * t0 La temperatura inicial para el algoritmo de recocido simulado.
 * t_step El factor por el cual la temperatura disminuye en cada iteraci√≥n.
 * max_iter_t_step El n√∫mero m√°ximo de iteraciones en cada paso de temperatura.
 * max_iters El n√∫mero m√°ximo de iteraciones para el algoritmo de recocido simulado.

> SA1: t0 = 2000, t_step = 0,90, max_iter_t_step = 100, max_iter = 1500

> SA2: t0 = 1500, t_step = 0,70, max_iter_t_step = 120, max_iter = 1500

> SA3: t0 = 1500, t_step = 0,85, max_iter_t_step = 100, max_iter = 1000

**Par√°metros del GRASP**:
 * max_iters El n√∫mero m√°ximo de iteraciones a realizar.
 * alpha El valor alfa utilizado para calcular la condici√≥n para RCL.

> Grasp 0.25-30: max_iters = 30, alpha = 0.25

> Grasp 0.5-30: max_iters = 30, alpha = 0.5

> Grasp 0.75-30: max_iters = 30, alpha = 0.75

> Grasp 0.25-60: max_iters = 60, alpha = 0.25

> Grasp 0.5-60: max_iters = 60, alpha = 0.5

> Grasp 0.75-60: max_iters = 60, alpha = 0.75

> Grasp 0.25-100: max_iters = 100, alpha = 0.25

> Grasp 0.5-100: max_iters = 100, alpha = 0.5

> Grasp 0.75-100: max_iters = 100, alpha = 0.75


**Par√°metros del Genetic Algorithm (GA) o Algoritmo Gen√©tico**:
 * population_size El tama√±o de la poblaci√≥n para el algoritmo gen√©tico.
 * mutation_rate el porcentaje en que ocurren las mutaciones durante el algoritmo gen√©tico.
 * max_iter El n√∫mero m√°ximo de iteraciones para el algoritmo gen√©tico.

> GA1: population_size = 50, mutation_rate = 5%, max_iter = 4000

> GA2: population_size = 50, mutation_rate = 10%, max_iter = 4000

> GA3: population_size = 100, mutation_rate = 5%, max_iter = 8000

### üìà Resultados
Los resultados obtenidos al ejecutar el programa con los casos de prueba en la carpeta `benchmarks` se encuentran en el directorio `results`, sin embargo, debido a la cantidad de datos obtenidos, se almacen√≥ los datos m√°s relevantes en el siguiente enlace: [Resultados](https://docs.google.com/spreadsheets/d/1hKiU8t9stOFJTKyNNQ1MKPmBr3OwZpMlk57sYdZWiEQ/edit#gid=294798782)

A modo de resumen y para facilitar la visualizaci√≥n de los resultados, se presentan las siguentes im√°genes comparativas resumidas a continuaci√≥n:

#### Promedio de diferencia entre la soluci√≥n obtenida y la soluci√≥n √≥ptima cada n tareas y m m√°quinas
- Resultados del corte anterior:
    - Soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n aleatoria.

![Diff Opt Corte1](./img/DiffOptCorte1.png)

- Resultados de Iterated Local Search (ILS), Tabu Search (TS) y Simulated Annealing (SA) usando diferentes par√°metros:

![Diff Opt ILS &TS &SA](./img/DiffOptILS&TS&SA.png)

- Resultados de GRASP usando diferentes par√°metros:

![Diff Opt GRASP](./img/DiffOptGRASP.png)

- Resultados de Genetic Algorithm (GA) usando diferentes par√°metros:

![Diff Op tGA](./img/DiffOptGA.png)

#### Resultados ordenados por promedio de diferencias entre la soluci√≥n obtenida y la soluci√≥n √≥ptima por n = 20

![Sorted Diff opt by n = 20](./img/SortedDiffn20Corte2.png)

#### Resultados ordenados por promedio de diferencias entre la soluci√≥n obtenida y la soluci√≥n √≥ptima por n = 25

![Sorted Diff opt by n = 25](./img/SortedDiffn25Corte2.png)

#### Promedio de tiempo en segundos para cada n tareas y m m√°quinas
- Resultados del corte anterior:
    - Soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n aleatoria.

![Time Corte1](./img/TimeCorte1.png)

- Resultados de Iterated Local Search (ILS), Tabu Search (TS) y Simulated Annealing (SA) usando diferentes par√°metros:

![Time ILS & TS & SA](./img/TimeILS&TS&SA.png)

- Resultados de GRASP usando diferentes par√°metros:

![Time GRASP](./img/TimeGRASP.png)

- Resultados de Genetic Algorithm (GA) usando diferentes par√°metros:

![Diff Op tGA](./img/TimeGA.png)

#### Resultados ordenados por promedio de tiempo por n = 20

![Sorted Time by n = 20](./img/SortedTimen20Corte2.png)

#### Resultados ordenados por promedio de tiempo por n = 25

![Sorted Time by n = 25](./img/SortedTimen25Corte2.png)


## üìå Conclusiones

- **Seg√∫n el promedio de diferencias entre la soluci√≥n √≥ptima y la soluci√≥n obtenida**:
    - **Para n = 20**:
        - *El algoritmo que di√≥ menor diferencia entre las soluciones √≥ptimas y las soluciones obtenidas* es **GRASP con alpha = 0.25** para el RCL **y con un m√°ximo de 100 iteraciones** con una diferencia de 1,2071.
        - *El algoritmo que di√≥ mayor diferencia entre las soluciones √≥ptimas y las soluciones obtenidas* es el **Algoritmo Gen√©tico con *population_size = 50, mutation_rate = 10%, max_iter = 4000* con una diferencia mayor que 150, espec√≠ficamente, 154,81156.
        - Adem√°s, en el caso del algoritmo gen√©tico, vemos que *GA1* y *GA2*, que tienen una diferencia mayor a 150, est√°n debajos de la b√∫squeda local con soluci√≥n inicial aleatoria que tiene una diferencia de 117,2631. As√≠ mismo, para este caso n=20, *GA1*, que se diferencia de *GA2* por tener un porcentaje de mutaci√≥n de 5% menos que *GA2*, vemos que *GA1* es mejor en cu√°nto a la diferencia con las soluciones √≥ptimas que *GA2*. Y del uso de los diferentes par√°metros para el algoritmo gen√©tico, vemos que *GA3* (con mayor cantidad de iteraciones y mayor tama√±o de poblaci√≥n con el mismo porcentaje de mutaci√≥n que *GA1*) es mejor que *GA1* y *GA2* e incluso de las soluciones obtenidas con la b√∫squeda local partiendo de la soluci√≥n inicial aleatoria.
    - **Para n = 25**:

## üìö Referencias
