# CI5652 - Identical Machine Scheduling
Proyecto de DiseÃ±o de Algoritmos II (CI5652) con soluciones aproximadas para el problema de Identical Machine Scheduling. Trimestre Abr-Jul 2024. Universidad SimÃ³n BolÃ­var.

## ğŸ“ Integrantes
- Ana Shek (19-10096)
- Santiago Finamore (18-10125)
- Jeamhowards Montiel (19-10234)

# ğŸ¤” DescripciÃ³n del problema
Dado un conjunto de `n` tareas y `m` mÃ¡quinas idÃ©nticas, el objetivo es asignar cada tarea a una mÃ¡quina y determinar la secuencia de tareas en cada mÃ¡quina de manera que se minimice la tardanza total (the total tardiness). Cada tarea `j` tiene un tiempo de procesamiento `p_j` y una fecha de vencimiento `d_j`. La tardanza de una tarea se calcula como `max(0, C_j - d_j)`, donde `C_j` es el tiempo de finalizaciÃ³n del trabajo `j`.

# ğŸ“‹ INFORME DEL PROYECTO - SEGUNDO CORTE
El programa estÃ¡ implementado en C++ y consta de los siguientes archivos para este segundo corte:

- `grasp.cpp`: Archivo principal del programa que contiene la implementaciÃ³n de una soluciÃ³n utilizando GRASP para el problema.

ğŸ“‚ En la carpeta `benchmarks` se encuentran los casos de pruebas de la primera corte del proyecto para medir y comparar el rendimiento de diferentes algoritmos para solucionar el problema descrito.

## DefiniciÃ³n de una perturbaciÃ³n e implementaciÃ³n de una bÃºsqueda local iterada (ILS).

### ğŸ”„ ILS

La implementaciÃ³n del algoritmo de BÃºsqueda Local Iterada (ILS) recibe la informaciÃ³n de las `n` tareas, la cantidad de `m` mÃ¡quinas, y varios parÃ¡metros que controlan el proceso de bÃºsqueda, como el nÃºmero mÃ¡ximo de iteraciones y la fuerza de perturbaciÃ³n inicial. A continuaciÃ³n se describen los pasos de la implementaciÃ³n:

1. **InicializaciÃ³n:** 
    - El algoritmo empieza creando una soluciÃ³n inicial `S` utilizando el mÃ©todo `mddScheduling`. 
    - Luego, aplica el algoritmo `RNA` a esta soluciÃ³n inicial para obtener una soluciÃ³n mejorada `S`.
    - Esta soluciÃ³n `S` es considerada la mejor soluciÃ³n conocida hasta el momento (`best_schedule`).

2. **Iteraciones principales:** 
    - Para cada iteraciÃ³n, se realiza una perturbaciÃ³n a la soluciÃ³n actual `current_schedule` aplicando un nÃºmero `p` de movimientos aleatorios.
    - DespuÃ©s de la perturbaciÃ³n, se aplica el algoritmo `RNA` para mejorar la soluciÃ³n perturbada.

3. **EvaluaciÃ³n de soluciones:** 
    - Si la soluciÃ³n mejorada tiene una tardanza total (`total_tardiness`) menor que la mejor soluciÃ³n conocida, se actualiza la mejor soluciÃ³n y se restablece la fuerza de perturbaciÃ³n `p` a su valor inicial.
    - Si no mejora, se incrementa un contador `i`.

4. **Ajuste de la perturbaciÃ³n:** 
    - Si el contador `i` alcanza el lÃ­mite `itermax`, se incrementa la fuerza de perturbaciÃ³n `p`. Si `p` excede un valor mÃ¡ximo `pmax`, se restablece a su valor inicial.

5. **RepeticiÃ³n:** 
    - Se repiten los pasos 2-4 hasta que se agoten las iteraciones.

El algoritmo `ILS` busca explorar el espacio de soluciones mediante la combinaciÃ³n de perturbaciones y optimizaciÃ³n local, ayudando a escapar de Ã³ptimos locales y encontrar soluciones mejores.

## DefiniciÃ³n de reglas para movimientos que han de ser tabÃºs e implementaciÃ³n de una bÃºsqueda tabÃº.

### ğŸš« Tabu Search

La implementaciÃ³n del algoritmo de BÃºsqueda TabÃº recibe la informaciÃ³n de las `n` tareas, la cantidad de `m` mÃ¡quinas, y varios parÃ¡metros que controlan el proceso de bÃºsqueda, como el nÃºmero mÃ¡ximo de iteraciones y la tenencia de la lista tabÃº. A continuaciÃ³n se describen los pasos de la implementaciÃ³n:

1. **InicializaciÃ³n:** 
    - El algoritmo empieza creando una soluciÃ³n inicial `S` utilizando el mÃ©todo `mddScheduling`. 
    - Esta soluciÃ³n `S` es considerada la mejor soluciÃ³n conocida hasta el momento (`best_schedule`).

2. **Lista TabÃº:** 
    - Se inicializa una lista tabÃº para almacenar los movimientos que estÃ¡n prohibidos temporalmente.

3. **Iteraciones principales:** 
    - Para cada iteraciÃ³n, se identifica la mÃ¡quina con mayor tardanza (`tardiest_machine`).
    - Se generan mÃºltiples vecinos de la soluciÃ³n actual mediante movimientos aleatorios en la mÃ¡quina con mayor tardanza.
    - Se selecciona el mejor vecino que no estÃ© en la lista tabÃº o que mejora la mejor soluciÃ³n conocida.

4. **ActualizaciÃ³n de la lista TabÃº:** 
    - Si el mejor vecino no estÃ¡ en la lista tabÃº o mejora la mejor soluciÃ³n conocida, se actualiza la soluciÃ³n actual y se aÃ±ade el movimiento a la lista tabÃº.
    - Si la lista tabÃº excede su tamaÃ±o mÃ¡ximo (`tabu_tenure`), se elimina el movimiento mÃ¡s antiguo.

5. **ActualizaciÃ³n de la mejor soluciÃ³n:** 
    - Si el vecino seleccionado mejora la mejor soluciÃ³n conocida, se actualiza la mejor soluciÃ³n.

6. **RepeticiÃ³n:** 
    - Se repiten los pasos 3-5 hasta que se agoten las iteraciones.

El algoritmo `Tabu Search` busca explorar el espacio de soluciones evitando ciclos y escapando de Ã³ptimos locales mediante el uso de una lista tabÃº que prohÃ­be ciertos movimientos temporalmente.

## DefiniciÃ³n de un proceso de enfriado progresivo e implementaciÃ³n de un recocido simulado.

### â„ï¸ Simulated Annealing

La implementaciÃ³n del algoritmo de Recocido Simulado (Simulated Annealing) recibe la informaciÃ³n de las `n` tareas, la cantidad de `m` mÃ¡quinas, y varios parÃ¡metros que controlan el proceso de bÃºsqueda, como la temperatura inicial y el factor de reducciÃ³n de temperatura. A continuaciÃ³n se describen los pasos de la implementaciÃ³n:

1. **InicializaciÃ³n:** 
    - El algoritmo empieza creando una soluciÃ³n inicial `S` utilizando el mÃ©todo `mddScheduling`. 
    - Esta soluciÃ³n `S` es considerada la mejor soluciÃ³n conocida hasta el momento (`best_schedule`).
    - Se inicializa la temperatura `t` con un valor inicial `t0`.

2. **Iteraciones principales:** 
    - Para cada iteraciÃ³n, se generan mÃºltiples vecinos de la soluciÃ³n actual mediante movimientos aleatorios.
    - Se calcula la diferencia de tardanza (`delta`) entre el vecino y la soluciÃ³n actual.

3. **Criterio de aceptaciÃ³n:** 
    - Si el vecino tiene una tardanza menor o igual, se acepta.
    - Si el vecino tiene una tardanza mayor, se acepta con una probabilidad `exp(-delta / t)`.

4. **ActualizaciÃ³n de la mejor soluciÃ³n:** 
    - Si la soluciÃ³n actual mejorada tiene una tardanza menor que la mejor soluciÃ³n conocida, se actualiza la mejor soluciÃ³n.

5. **ReducciÃ³n de la temperatura:** 
    - DespuÃ©s de un nÃºmero fijo de iteraciones, se reduce la temperatura multiplicÃ¡ndola por un factor `t_step`.
    - Si la temperatura cae por debajo de un umbral `epsilon`, se restablece a su valor inicial `t0`.

6. **RepeticiÃ³n:** 
    - Se repiten los pasos 2-5 hasta que se agoten las iteraciones.

El algoritmo `Simulated Annealing` busca explorar el espacio de soluciones permitiendo peores soluciones con una probabilidad decreciente, lo que ayuda a escapar de Ã³ptimos locales y encontrar mejores soluciones globales.

## ğŸ² DefiniciÃ³n de un mÃ©todo de construciÃ³n para una RCL e implementaciÃ³n de GRASP.

### ğŸ‘¨â€âš–ï¸ MÃ©todo de construciÃ³n para una RCL

Para la definiciÃ³n del RCL en este problema, se utilizÃ³ el enfoque heurÃ­stico Modified Due Date (MDD) explicado en el primer corte del proyecto:

1. Sea `S` una soluciÃ³n parcialmente construida.
2. Se tiene una lista de tareas no programadas `U` en la soluciÃ³n `S`.
3. Para cada mÃ¡quina `j`, dividir `U` en dos subconjuntos `U1j` y `U2j` para `j` = 1, 2, ..., `m`.
    > `U1j` contiene las tareas que no se pueden completar en su fecha de vencimiento en la mÃ¡quina `j`.

    > `U2j` contiene las tareas que sÃ­ se pueden completar antes de su fecha de vencimiento en la mÃ¡quina `j`.
4. De `U1j` y `U2j`, encontrar los subconjuntos `Î³j` y `Î»j` que contienen las tareas con el tiempo de procesamiento mÃ­nimo y la fecha de vencimiento mÃ­nima, respectivamente. 
5. Seleccionar una tarea `gj` de `Î³j` o `Î»j` que minimice el valor de MDD en la mÃ¡quina `j`. El valor de MDD en la mÃ¡quina `j` de una tarea `i` estÃ¡ dada por `MDD(j, i) = max(Cj + pi, di)` 
    > `Cj` es la suma del procesamiento de tiempo de las tareas que ya han sido programados en la mÃ¡quina `j`.

    > `pi` es el tiempo de procesamiento de la tarea `i` con su fecha de vencimiento `di`.
6. Luego, sea `C` el conjunto resultante de tener cada par `<g, l>`, donde cada una de estos pares representa que la tarea `g` es la tarea que produce el menor valor MDD en la mÃ¡quina `l`).
7. Se puede definir el costo de la funciÃ³n de un elemento `<g, l>` en `C` como `c(<g, l>) = MDD(g, l)`.
8. TambiÃ©n se define `c_min = min{ c(<g, l>) | <g, l> âˆˆ C}` y `c_max = max{ c(<g, l>) | <g, l> âˆˆ C}`.
9. Entonces, el `RCL = { <g, l> âˆˆ C | c(<g, l>) <= c_min + Î±(c_max - c_min)}`

### ğŸ° GRASP

La implementaciÃ³n del algoritmo GRASP recibe la informaciÃ³n de las `n` tareas, la cantidad de las `m` mÃ¡quinas, el `alpha` para el RCL y `el mÃ¡ximo nÃºmero de iteraciones`. A continuaciÃ³n se describen los pasos de la implementaciÃ³n: 

1. El algoritmo empieza a generar una soluciÃ³n inicial aleatoria `S`. Por ahora, se tiene que `S` es la mejor soluciÃ³n que se tiene para el problema.
2. Ahora, para cada iteraciÃ³n, se construye una soluciÃ³n voraz aleatoria `S'`, el cual: 

    2.1. Se considera las tareas que aÃºn no han sido programadas y se aplica el enfoque heurÃ­stico para la Lista Restringida de Candidatos (RCL). 

    2.2. De esta lista RCL, se escoge aleatoriamente un elemento: `<g, l>`. 

    2.3. Luego, del elemento seleccionado `<g, l>`, se le asigna la tarea `g` a la mÃ¡quina `l`. 

    2.4. Se elimina la tarea `g` de la lista de tareas sin programar en esta soluciÃ³n parcial construida `S'`.

    2.5. Se repite los pasos 2.1 a 2.4 hasta que no queden tareas sin programar.
3. Se reemplaza `S` por `S'` si el retraso total o total tardiness de las tareas en la soluciÃ³n `S` es mayor que el de `S'`, en caso contrario, no se hace nada.
4. Se repite el paso 2 y 3 hasta que se acaben las iteraciones.

## DefiniciÃ³n de un fenotipo/genotipo, operadores de cruce y mutaciÃ³n e implementaciÃ³n de un algoritmo genÃ©tico.

## ğŸš€ Uso

Para compilar y ejecutar el programa, se debe ejecutar los siguientes comandos en la terminal:

```bash
make
```

```bash
cd target
```

```bash
./PROY2 <path_to_benchmarks> <algorithm>
```

Donde `<path_to_benchmarks>` es la ruta a la carpeta que contiene los casos de prueba y `<algorithm>` es el nÃºmero del algoritmo a ejecutar:

1. BÃºsqueda Local Iterada (ILS)
2. BÃºsqueda TabÃº 
3. Reconocido Simulado
4. GRASP
5. Algoritmo GenÃ©tico

## ğŸ“„ Analisis de resultados

A continuaciÃ³n se presenta el anÃ¡lisis de los resultados obtenidos al ejecutar el programa con los casos de prueba en la carpeta `benchmarks` en una **laptop** con **procesador AMD Ryzen 5 5500U**, **disco SSD**, **8GB de memoria RAM** y **WSL 2 Ubuntu**. Se tomarÃ¡ en cuenta los valores de tardanza Ã³ptima obtenidos del paper de referencia de donde se obtuvo el `benchmark` para comparar los resultados obtenidos con los algoritmos implementados.


### ğŸ“Š MÃ©tricas Comparativas

Las mÃ©tricas claves en el anÃ¡lisis incluyen:

- **Tardanza Total (Total Tardiness)**: La suma de las tardanzas de todas las tareas.
- **Diferencia con la SoluciÃ³n Ã“ptima (Optimal Solution Difference)**: La diferencia entre la soluciÃ³n obtenida y la soluciÃ³n Ã³ptima.

### ğŸ“ˆ Resultados


## ğŸ“Œ Conclusiones



## ğŸ“š Referencias
