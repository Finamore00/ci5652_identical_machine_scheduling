# CI5652 - Identical Machine Scheduling
Proyecto de Dise√±o de Algoritmos II (CI5652) con soluciones aproximadas para el problema de Identical Machine Scheduling. Trimestre Abr-Jul 2024. Universidad Sim√≥n Bol√≠var.

## üìù Integrantes
- Ana Shek (19-10096)
- Santiago Finamore (18-10125)
- Jeamhowards Montiel (19-10234)

# ü§î Descripci√≥n del problema
Dado un conjunto de `n` tareas y `m` m√°quinas id√©nticas, el objetivo es asignar cada tarea a una m√°quina y determinar la secuencia de tareas en cada m√°quina de manera que se minimice la tardanza total (the total tardiness). Cada tarea `j` tiene un tiempo de procesamiento `p_j` y una fecha de vencimiento `d_j`. La tardanza de una tarea se calcula como `max(0, C_j - d_j)`, donde `C_j` es el tiempo de finalizaci√≥n del trabajo `j`.

# üìã INFORME DEL PROYECTO - SEGUNDO CORTE
El programa est√° implementado en C++ y consta de los siguientes archivos para este segundo corte:

- `grasp.cpp`: Archivo principal del programa que contiene la implementaci√≥n de una soluci√≥n utilizando GRASP para el problema.
- `evolution.cpp`: Archivo que contiene los tipos de datos y algoritmos requeridos por la implementaci√≥n del algoritmo gen√©tico para la reosluci√≥n del problema.

üìÇ En la carpeta `benchmarks` se encuentran los casos de pruebas de la primera corte del proyecto para medir y comparar el rendimiento de diferentes algoritmos para solucionar el problema descrito.

## Definici√≥n de una perturbaci√≥n e implementaci√≥n de una b√∫squeda local iterada (ILS).

### üîÑ ILS

La implementaci√≥n del algoritmo de B√∫squeda Local Iterada (ILS) recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como el n√∫mero m√°ximo de iteraciones y la fuerza de perturbaci√≥n inicial. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Luego, aplica el algoritmo `local search` a esta soluci√≥n inicial para obtener una soluci√≥n mejorada `S`.
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).

2. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se realiza una perturbaci√≥n a la soluci√≥n actual `current_schedule` aplicando un n√∫mero `p` de movimientos aleatorios.
    - Despu√©s de la perturbaci√≥n, se aplica el algoritmo `local search` para mejorar la soluci√≥n perturbada.

3. **Evaluaci√≥n de soluciones:** 
    - Si la soluci√≥n mejorada tiene una tardanza total (`total_tardiness`) menor que la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n y se restablece la fuerza de perturbaci√≥n `p` a su valor inicial.
    - Si no mejora, se incrementa un contador `i`.

4. **Ajuste de la perturbaci√≥n:** 
    - Si el contador `i` alcanza el l√≠mite `itermax`, se incrementa la fuerza de perturbaci√≥n `p`. Si `p` excede un valor m√°ximo `pmax`, se restablece a su valor inicial.

5. **Repetici√≥n:** 
    - Se repiten los pasos 2-4 hasta que se agoten las iteraciones.

El algoritmo `ILS` busca explorar el espacio de soluciones mediante la combinaci√≥n de perturbaciones y optimizaci√≥n local, ayudando a escapar de √≥ptimos locales y encontrar soluciones mejores.

## Definici√≥n de reglas para movimientos que han de ser tab√∫s e implementaci√≥n de una b√∫squeda tab√∫.

### üö´ Tabu Search

La implementaci√≥n del algoritmo de B√∫squeda Tab√∫ recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como el n√∫mero m√°ximo de iteraciones y la tenencia de la lista tab√∫. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).

2. **Lista Tab√∫:** 
    - Se inicializa una lista tab√∫ para almacenar los movimientos que est√°n prohibidos temporalmente.

3. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se identifica la m√°quina con mayor tardanza (`tardiest_machine`).
    - Se generan m√∫ltiples vecinos de la soluci√≥n actual mediante movimientos aleatorios en la m√°quina con mayor tardanza.
    - Se selecciona el mejor vecino que no est√© en la lista tab√∫ o que mejora la mejor soluci√≥n conocida.

4. **Actualizaci√≥n de la lista Tab√∫:** 
    - Si el mejor vecino no est√° en la lista tab√∫ o mejora la mejor soluci√≥n conocida, se actualiza la soluci√≥n actual y se a√±ade uno de los √≠ndices de los trabajos restantes de la maquina con m√°s tardiness a la lista tab√∫, esto para asegurar que no se extraigan varias veces elementos de la misma maquina, evitando as√≠ ciclos.
    - Si la lista tab√∫ excede su tama√±o m√°ximo (`tabu_tenure`), se elimina el movimiento m√°s antiguo.

5. **Actualizaci√≥n de la mejor soluci√≥n:** 
    - Si el vecino seleccionado mejora la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n.

6. **Repetici√≥n:** 
    - Se repiten los pasos 3-5 hasta que se agoten las iteraciones.

El algoritmo `Tabu Search` busca explorar el espacio de soluciones evitando ciclos y escapando de √≥ptimos locales mediante el uso de una lista tab√∫ que proh√≠be ciertos movimientos temporalmente.

## Definici√≥n de un proceso de enfriado progresivo e implementaci√≥n de un recocido simulado.

### ‚ùÑÔ∏è Simulated Annealing

La implementaci√≥n del algoritmo de Recocido Simulado (Simulated Annealing) recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como la temperatura inicial y el factor de reducci√≥n de temperatura. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).
    - Se inicializa la temperatura `t` con un valor inicial `t0`.

2. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se generan m√∫ltiples vecinos de la soluci√≥n actual mediante movimientos aleatorios.
    - Se calcula la diferencia de tardanza (`delta`) entre el vecino y la soluci√≥n actual.

3. **Criterio de aceptaci√≥n:** 
    - Si el vecino tiene una tardanza menor o igual, se acepta.
    - Si el vecino tiene una tardanza mayor, se acepta con una probabilidad `exp(-delta / t)`.

4. **Actualizaci√≥n de la mejor soluci√≥n:** 
    - Si la soluci√≥n actual mejorada tiene una tardanza menor que la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n.

5. **Reducci√≥n de la temperatura:** 
    - Despu√©s de un n√∫mero fijo de iteraciones, se reduce la temperatura multiplic√°ndola por un factor `t_step`.
    - Si la temperatura cae por debajo de un umbral `epsilon`, se restablece a su valor inicial `t0`.

6. **Repetici√≥n:** 
    - Se repiten los pasos 2-5 hasta que se agoten las iteraciones.

El algoritmo `Simulated Annealing` busca explorar el espacio de soluciones permitiendo peores soluciones con una probabilidad decreciente, lo que ayuda a escapar de √≥ptimos locales y encontrar mejores soluciones globales.

## üé≤ Definici√≥n de un m√©todo de construci√≥n para una RCL e implementaci√≥n de GRASP.

### üë®‚Äç‚öñÔ∏è M√©todo de construci√≥n para una RCL

Para la definici√≥n del RCL en este problema, se utiliz√≥ el enfoque heur√≠stico Modified Due Date (MDD) explicado en el primer corte del proyecto:

1. Sea `S` una soluci√≥n parcialmente construida.
2. Se tiene una lista de tareas no programadas `U` en la soluci√≥n `S`.
3. Para cada m√°quina `j`, dividir `U` en dos subconjuntos `U1j` y `U2j` para `j` = 1, 2, ..., `m`.
    > `U1j` contiene las tareas que no se pueden completar en su fecha de vencimiento en la m√°quina `j`.

    > `U2j` contiene las tareas que s√≠ se pueden completar antes de su fecha de vencimiento en la m√°quina `j`.
4. De `U1j` y `U2j`, encontrar los subconjuntos `Œ≥j` y `Œªj` que contienen las tareas con el tiempo de procesamiento m√≠nimo y la fecha de vencimiento m√≠nima, respectivamente. 
5. Seleccionar una tarea `gj` de `Œ≥j` o `Œªj` que minimice el valor de MDD en la m√°quina `j`. El valor de MDD en la m√°quina `j` de una tarea `i` est√° dada por `MDD(j, i) = max(Cj + pi, di)` 
    > `Cj` es la suma del procesamiento de tiempo de las tareas que ya han sido programados en la m√°quina `j`.

    > `pi` es el tiempo de procesamiento de la tarea `i` con su fecha de vencimiento `di`.
6. Luego, sea `C` el conjunto resultante de tener cada par `<g, l>`, donde cada una de estos pares representa que la tarea `g` es la tarea que produce el menor valor MDD en la m√°quina `l`).
7. Se puede definir el costo de la funci√≥n de un elemento `<g, l>` en `C` como `c(<g, l>) = MDD(g, l)`.
8. Tambi√©n se define `c_min = min{ c(<g, l>) | <g, l> ‚àà C}` y `c_max = max{ c(<g, l>) | <g, l> ‚àà C}`.
9. Entonces, el `RCL = { <g, l> ‚àà C | c(<g, l>) <= c_min + Œ±(c_max - c_min)}`

### üé∞ GRASP

La implementaci√≥n del algoritmo GRASP recibe la informaci√≥n de las `n` tareas, la cantidad de las `m` m√°quinas, el `alpha` para el RCL y `el m√°ximo n√∫mero de iteraciones`. A continuaci√≥n se describen los pasos de la implementaci√≥n: 

1. El algoritmo empieza a generar una soluci√≥n inicial aleatoria `S`. Por ahora, se tiene que `S` es la mejor soluci√≥n que se tiene para el problema.
2. Ahora, para cada iteraci√≥n, se construye una soluci√≥n voraz aleatoria `S'`, el cual: 

    2.1. Se considera las tareas que a√∫n no han sido programadas y se aplica el enfoque heur√≠stico para la Lista Restringida de Candidatos (RCL). 

    2.2. De esta lista RCL, se escoge aleatoriamente un elemento: `<g, l>`. 

    2.3. Luego, del elemento seleccionado `<g, l>`, se le asigna la tarea `g` a la m√°quina `l`. 

    2.4. Se elimina la tarea `g` de la lista de tareas sin programar en esta soluci√≥n parcial construida `S'`.

    2.5. Se repite los pasos 2.1 a 2.4 hasta que no queden tareas sin programar.
3. Se reemplaza `S` por `S'` si el retraso total o total tardiness de las tareas en la soluci√≥n `S` es mayor que el de `S'`, en caso contrario, no se hace nada.
4. Se repite el paso 2 y 3 hasta que se acaben las iteraciones.

## Definici√≥n de un fenotipo/genotipo, operadores de cruce y mutaci√≥n e implementaci√≥n de un algoritmo gen√©tico.

### üß¨ Genotipo de los individuos para modelado del problema 

Para prop√≥sitos de nuestro problema, definimos un gen como un par ordenado que contiene el identificador num√©rico de un trabajo y el n√∫mero de la m√°quina donde este trabajo est√° asignado en el cronograma que modela su conjunto de genes. Esto es, si tenemos, por ejemplo, la tupla (5, 8), esto representa que el trabajo de ID 5 est√° asignado a la m√°quina 8. Este modelado de un gen se encuentra en el struct `Gene` definido en `evolution.cpp`.

Como por la definici√≥n del problema cada trabajo est√° asignado a una sola m√°quina, para una instancia del mismo con `n` trabajos el genotipo de un individuo viene dado por un vector de `n` genes, donde cada posici√≥n indica la m√°quina a la que est√° asignada cada uno de los trabajos. De esta manera, el genotipo de un individuo est√° restringido en el hecho de que para cada trabajo en la instancia del problema su identificador aparece en el genotipo de un individuo exactamente una vez. En referencia al *orden* que tienen los trabajos dentro de cada m√°quina individual, el mismo viene dado por el orden que tienen los trabajos involucrados dentro del vector genotipo. Esto es, si para un individuo tenemos el vector de genes `[..., (6, 1), ..., (9, 1), ..., (4, 1), ...]` entonces en la m√°quina 1 estar√°n ubicados los trabajos de identificadores 6, 9 y 4 en ese orden.

De esta manera una poblaci√≥n no es m√°s que un arreglo de genotipos, que a su vez son arreglos de genes. Los tipos de datos para la representaci√≥n de un individuo y de una poblaci√≥n se encuentran en los tipos `Individual` y `Population` definidos en `evolution.cpp`.

### üëæ Fenotipo de los individuos y funci√≥n de *fitness*

El fenotipo de los individuos viene dado por el mismo modelo de cronograma utilizado en el primer corte. Esto es, un cronograma es un arreglo multi-dimensional donde cada posici√≥n representa a una de las m√°quinas disponibles, y cada m√°quina es un arreglo de trabajos que indica cu√°les trabajos est√°n asignados a esa m√°quina y en qu√© orden se ejecutar√°n dentro de la misma. La obtensi√≥n del fenotipo de un individuo a trav√©s de su genotipo es prove√≠da por la funci√≥n `get_fenotype` definida en `evolution.cpp`.

La aptitud de un individuo viene dada directamente por la morosidad total de su fenotipo, la cual se calcula de la misma forma presentada en el corte pasado. Como los algoritmos gen√©ticos son inherentemente problemas de maximizaci√≥n, y estamos ante un problema que busca un m√≠nimo global, la definici√≥n espec√≠fica de la aptitud de un individuo viene dada por el negativo de su morosidad total.

### üë®‚Äçüë©‚Äçüë¶ Selecci√≥n de Padres y apareamiento

El algoritmo implementado hace uso de apareamiento con 2 padres. Cada individuo tiene a su vez una probabilidad asociada de ser escogido para ser padre de la siguiente generaci√≥n igual al cociente de su valor para la funci√≥n de aptitud entre la suma total de las aptitudes de todos los individuos. Esto es, si definimos f(i) como la aptitud del individuo i de la poblaci√≥n S, entonces la probabilidad p(i) de que i sea escogido como padre viene dada por:

<center> <code> p(i) = 1 - f(i)/sum(f(j) for j in S) </code> </center>

Tras ser escogidos dos padres, sus descendientes son obtenidos utilizando cruce parcialmente mapeado. Esto debido a que, como cada identificador de trabajo aparece exactamente una vez dentro del genotipo de cualquier individuo, podemos, de cierta manera, tratar los genotipos como permutaciones de los identificadores de los trabajos. 

La selecci√≥n de padres y el operador de cruce vienen dados por las funciones `choose_parents` y `partially_mapped_crossover` definidas en evolution.cpp, respectivamente.

### ‚ò¢Ô∏è Mutaci√≥n

La mutaci√≥n de individuos se realiza con probabilidad uniforme sobre los genes de los mismos. La probabilidad de mutaci√≥n es prove√≠da como argumento de entrada al algoritmo gen√©tico (`mutation_rate`) y no var√≠a de ninguna manera durante la ejecuci√≥n del algoritmo. El operador de mutaci√≥n implementado recibe un gen y reasigna el trabajo encontrado dentro del mismo a otra m√°quina aleatoriamente escogida. El operador de mutaci√≥n no modifica de ninguna manera los identificadores de los trabajos encontrados en los genes ni efect√∫a ning√∫n tipo de reordenamiento sobre los genes del fenotipo.

## üöÄ Uso

Para compilar y ejecutar el programa, se debe ejecutar los siguientes comandos en la terminal:

```bash
make
```

```bash
cd target
```

```bash
./PROY2 <path_to_benchmarks> <algorithm>
```

Donde `<path_to_benchmarks>` es la ruta a la carpeta que contiene los casos de prueba y `<algorithm>` es el n√∫mero del algoritmo a ejecutar:

1. B√∫squeda Local Iterada (ILS)
2. B√∫squeda Tab√∫ 
3. Reconocido Simulado
4. GRASP
5. Algoritmo Gen√©tico

## üìÑ Analisis de resultados

A continuaci√≥n se presenta el an√°lisis de los resultados obtenidos al ejecutar el programa con los casos de prueba en la carpeta `benchmarks` en una **laptop** con **procesador AMD Ryzen 5 5500U**, **disco SSD**, **8GB de memoria RAM** y **WSL 2 Ubuntu**. Se tomar√° en cuenta los valores de tardanza √≥ptima obtenidos del paper de referencia de donde se obtuvo el `benchmark` para comparar los resultados obtenidos con los algoritmos implementados de ambos cortes del proyecto.


### üìä M√©tricas Comparativas
Las m√©tricas claves en el an√°lisis incluyen:

- **Tardanza Total (Total Tardiness)**: La suma de las tardanzas de todas las tareas.
- **Diferencia con la Soluci√≥n √ìptima (Optimal Solution Difference)**: La diferencia entre la soluci√≥n obtenida y la soluci√≥n √≥ptima.
- **Tiempo en segundos (Time in seconds)**: El tiempo en segundos tomado por el algoritmo.

As√≠ mismo, se emplearon diferentes par√°metros para cada algoritmo implementado en este segundo corte:
**Par√°metros del Iterated Local Search (ILS)**:
 * max_iter Cantidad m√°xima de iteraciones para el algoritmo ILS
 * p0 La fuerza de perturbaci√≥n inicial.
 * pmax El multiplicador m√°ximo de fuerza de perturbaci√≥n.
 * lsmax El n√∫mero m√°ximo de iteraciones para el algoritmo de local search dentro de ILS.
 * itermax El n√∫mero m√°ximo de iteraciones antes de aumentar la fuerza de la perturbaci√≥n.

> ILS1: max_iter = 1500, p0 = 10, pmax = 4, lsmax = 100, itermax = 100

> ILS2: max_iter = 1000, p0 = 3, pmax = 15, lsmax = 70, itermax = 150

**Par√°metros del Tabu Search (TS)**:
 * max_iter El n√∫mero m√°ximo de iteraciones para el algoritmo de b√∫squeda tab√∫.
 * max_grn_iter El n√∫mero m√°ximo de iteraciones para generar vecinos dentro de cada iteraci√≥n.
 * tabu_tenure El tama√±o de la lista tab√∫.

> TS1: max_iter = 10000, max_grn_iter = 100, tabu_ternure = 7

> TS2: max_iter = 6000, max_grn_iter = 70, tabu_ternure = 5

**Par√°metros del Simulated Annealing (SA) o Reconocido Simulado**:
 * t0 La temperatura inicial para el algoritmo de recocido simulado.
 * t_step El factor por el cual la temperatura disminuye en cada iteraci√≥n.
 * max_iter_t_step El n√∫mero m√°ximo de iteraciones en cada paso de temperatura.
 * max_iters El n√∫mero m√°ximo de iteraciones para el algoritmo de recocido simulado.

> SA1: t0 = 2000, t_step = 0,90, max_iter_t_step = 100, max_iter = 1500

> SA2: t0 = 1500, t_step = 0,70, max_iter_t_step = 120, max_iter = 1500

> SA3: t0 = 1500, t_step = 0,85, max_iter_t_step = 100, max_iter = 1000

**Par√°metros del GRASP**:
 * max_iters El n√∫mero m√°ximo de iteraciones a realizar.
 * alpha El valor alfa utilizado para calcular la condici√≥n para RCL.

> Grasp 0.25-30: max_iters = 30, alpha = 0.25

> Grasp 0.5-30: max_iters = 30, alpha = 0.5

> Grasp 0.75-30: max_iters = 30, alpha = 0.75

> Grasp 0.25-60: max_iters = 60, alpha = 0.25

> Grasp 0.5-60: max_iters = 60, alpha = 0.5

> Grasp 0.75-60: max_iters = 60, alpha = 0.75

> Grasp 0.25-100: max_iters = 100, alpha = 0.25

> Grasp 0.5-100: max_iters = 100, alpha = 0.5

> Grasp 0.75-100: max_iters = 100, alpha = 0.75


**Par√°metros del Genetic Algorithm (GA) o Algoritmo Gen√©tico**:
 * population_size El tama√±o de la poblaci√≥n para el algoritmo gen√©tico.
 * mutation_rate el porcentaje en que ocurren las mutaciones durante el algoritmo gen√©tico.
 * max_iter El n√∫mero m√°ximo de iteraciones para el algoritmo gen√©tico.

> GA1: population_size = 50, mutation_rate = 5%, max_iter = 4000

> GA2: population_size = 50, mutation_rate = 10%, max_iter = 4000

> GA3: population_size = 100, mutation_rate = 5%, max_iter = 8000

### üìà Resultados
Los resultados obtenidos al ejecutar el programa con los casos de prueba en la carpeta `benchmarks` se encuentran en el directorio `results`, sin embargo, debido a la cantidad de datos obtenidos, se almacen√≥ los datos m√°s relevantes en el siguiente enlace: [Resultados](https://docs.google.com/spreadsheets/d/1hKiU8t9stOFJTKyNNQ1MKPmBr3OwZpMlk57sYdZWiEQ/edit#gid=294798782)

A modo de resumen y para facilitar la visualizaci√≥n de los resultados, se presentan las siguentes im√°genes comparativas resumidas a continuaci√≥n:

#### Promedio de diferencia entre la soluci√≥n obtenida y la soluci√≥n √≥ptima cada n tareas y m m√°quinas
- Resultados del corte anterior:
    - Soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n aleatoria.

![Diff Opt Corte1](./img/DiffOptCorte1.png)

- Resultados de Iterated Local Search (ILS), Tabu Search (TS) y Simulated Annealing (SA) usando diferentes par√°metros:

![Diff Opt ILS &TS &SA](./img/DiffOptILS&TS&SA.png)

- Resultados de GRASP usando diferentes par√°metros:

![Diff Opt GRASP](./img/DiffOptGRASP.png)

- Resultados de Genetic Algorithm (GA) usando diferentes par√°metros:

![Diff Op tGA](./img/DiffOptGA.png)

#### Resultados ordenados por promedio de diferencias entre la soluci√≥n obtenida y la soluci√≥n √≥ptima por n = 20

![Sorted Diff opt by n = 20](./img/SortedDiffn20Corte2.png)

#### Resultados ordenados por promedio de diferencias entre la soluci√≥n obtenida y la soluci√≥n √≥ptima por n = 25

![Sorted Diff opt by n = 25](./img/SortedDiffn25Corte2.png)

#### Promedio de tiempo en segundos para cada n tareas y m m√°quinas
- Resultados del corte anterior:
    - Soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n heur√≠stica
    
    - Soluci√≥n de b√∫squeda local partiendo de una soluci√≥n aleatoria.

![Time Corte1](./img/TimeCorte1.png)

- Resultados de Iterated Local Search (ILS), Tabu Search (TS) y Simulated Annealing (SA) usando diferentes par√°metros:

![Time ILS & TS & SA](./img/TimeILS&TS&SA.png)

- Resultados de GRASP usando diferentes par√°metros:

![Time GRASP](./img/TimeGRASP.png)

- Resultados de Genetic Algorithm (GA) usando diferentes par√°metros:

![Diff Op tGA](./img/TimeGA.png)

#### Resultados ordenados por promedio de tiempo por n = 20

![Sorted Time by n = 20](./img/SortedTimen20Corte2.png)

#### Resultados ordenados por promedio de tiempo por n = 25

![Sorted Time by n = 25](./img/SortedTimen25Corte2.png)


## üìå Conclusiones

- **Seg√∫n el promedio de diferencias entre la soluci√≥n √≥ptima y la soluci√≥n obtenida**:
    - **Para n = 20** (segun la lista de resultados ordenados por promedio de diferencias entre la soluci√≥n obtenida y la soluci√≥n √≥ptima por n = 20):
        - *El algoritmo que di√≥ menor diferencia entre las soluciones √≥ptimas y las soluciones obtenidas* es **GRASP con alpha = 0.25** para el RCL **y con un m√°ximo de 100 iteraciones**, cuyo promedio de diferencia es de 1,2071.
        - *El algoritmo que di√≥ mayor diferencia entre las soluciones √≥ptimas y las soluciones obtenidas* es el **Algoritmo Gen√©tico con *population_size = 50, mutation_rate = 10%, max_iter = 4000** con un promedio de diferencia mayor que 150, espec√≠ficamente, 154,81156.
        - Adem√°s, en el caso del algoritmo gen√©tico, vemos que *GA1* y *GA2*, que tienen un promedio de diferencia mayor a 150, est√°n debajos de la b√∫squeda local con soluci√≥n inicial aleatoria que tiene un promedio de diferencia de 117,2631. As√≠ mismo, para este caso n=20, *GA1*, que se diferencia de *GA2* por tener un porcentaje de mutaci√≥n de 5% menos que *GA2*, vemos que *GA1* es mejor en cu√°nto a la diferencia con las soluciones √≥ptimas que *GA2*. Y del uso de los diferentes par√°metros para el algoritmo gen√©tico, vemos que *GA3* (con mayor cantidad de iteraciones y mayor tama√±o de poblaci√≥n con el mismo porcentaje de mutaci√≥n que *GA1*) es mejor que *GA1* y *GA2* e incluso de las soluciones obtenidas con la b√∫squeda local partiendo de la soluci√≥n inicial aleatoria.
        - Luego, **todos los que est√°n encima de *GA3* tienen un promedio de diferencia menor de 20**, siendo *la soluci√≥n heur√≠stica* el que est√° encima de *GA3*.
        - Justamente encima de la soluci√≥n heur√≠stica, se encuentra el algoritmo de *Reconocido Simulado* usando los 3 diferentes par√°metros, vemos que *SA2* tiene menor diferencia con la soluci√≥n √≥ptima en comparaci√≥n con *SA1* y *SA3*, lo que sugiere que aunque *SA2* tiene una temperatura inicial y el t_step (factor el cual la temperatura disminuye en cada iteraci√≥n) menor o igual que las otras dos, lo importante es que tiene un max_iter_t_step mayor (el n√∫mero max de iteraciones en cada paso de temperatura).
        - Para el ILS, se observa que ambos ILSs est√°n justos encima de *SA2*, siendo el mejor entre las dos, el *ILS2*, que aunque tiene menor cantidad de iteraciones m√°ximas para el algoritmo ILS y tambi√©n la menor cantidad de iteraciones para el algoritmo local search que trabaja dentro del ILS, se tiene que produce mejores resultados si el ILS tiene un mayor multiplicador m√°ximo de fuerza de perturbaci√≥n, as√≠ como tambi√©n un mayor n√∫mero de iteraciones antes de aumentar la fuerza de perturbaci√≥n junto con una menor fuerza de perturbaci√≥n inicial. 
        - *Arriba del *ILS2**, se encuentra *la b√∫squeda local a partir de una soluci√≥n inicial heur√≠stica*, con una diferencia con el *ILS2* de 0,89.
        - Las dos b√∫squedas locales, ***TS1* y *TS2* se presentan encima de la soluci√≥n con *b√∫squeda local partiendo de una soluci√≥n inicial heur√≠stica***, el cual *TS2* tiene menos diferencia con las soluciones √≥ptimas que *TS1*. De esto, se sugiere que a pesar de que *TS2* tiene valores menores para la cantidad m√°xima de iteraciones, el n√∫mero m√°ximo de iteraciones para generar vecinos dentro de cada iteraci√≥n (max_gnr_iter) y el n√∫mero de iteraciones durante las cuales un movimiento permanece en la lista tab√∫ (tabu_tenure), una configuraci√≥n m√°s peque√±a en estos par√°metros puede conducir a un mejor rendimiento del algoritmo de b√∫squeda tab√∫.
        - Y por √∫ltimo, las 9 diferentes configuraciones del **algoritmo GRASP** est√°n encima de cualquier otro algoritmo. Y seg√∫n los resultados, parece que **para un alpha peque√±o, como 0.25, ofrece mejores soluciones al problema para cualquier cantidad de iteraciones**.

    - **Para n = 25** (segun la lista de resultados ordenados por promedio de diferencias entre la soluci√≥n obtenida y la soluci√≥n √≥ptima por n = 20):
        - Para este caso, vemos que **el que ofrece mejores soluciones al problema sigue siendo *el algoritmo GRASP con alpha = 0.25 y un m√°ximo de 100 iteraciones***. 
        - As√≠ mismo, el orden desde *GRASP 0.25-100* hasta *ILS1* permanece igual que para el caso de n = 20.
        - Los √∫ltimos tres de la lista son justamente las tres configuraciones diferentes del algoritmo gen√©tico, el cual tiene un promedio de diferencias mayores que 170, donde *GA3* sigue siendo mejor que *GA1* y *GA2*. Estas √∫ltimas dos tienen un promedio de diferencias mayores que 230. 
        - Sin embargo, ahora *GA3* est√° debajo de la soluci√≥n de b√∫squeda local con soluci√≥n inicial aleatoria. Adem√°s, *GA2* est√° encima de *GA1*, lo cual parece sugerir que para una cantidad de tareas (n) mayor, es preferible tener un porcentaje de mutaci√≥n m√°s grande.
        - **La soluci√≥n heur√≠stica sigue siendo mejor que los algoritmos gen√©ticos y la b√∫squeda local con soluci√≥n inicial aleatoria**.
        - Para el caso de las 3 configuraciones del algoritmo simulado reconocido (SA), las tres siguen estando encima de la soluci√≥n heur√≠stica, pero ahora se tiene que *SA2* est√° debajo tanto de *SA1* como de *SA3*. De estas 3 configuraciones de SA, el mejor es *SA1*, el cual tiene valores mayores para la temperatura inicial, el factor de decremento de la temperatura y el n√∫mero m√°ximo de iteraciones para el algoritmo.

- **Seg√∫n el tiempo promedio de ejecuci√≥n**
    - **Para n=20:**
        - El algoritmo m√°s r√°pido es la heur√≠stica, con un tiempo de 0.00037 segundos, lo que lo hace extremadamente eficiente en t√©rminos de tiempo. *El algoritmo GRASP* presenta un buen balance entre *tiempo de ejecuci√≥n* y *calidad de soluci√≥n*. Por ejemplo, *Grasp 0.25-30* tiene un tiempo de ejecuci√≥n de 0.61 segundos, siendo uno de los m√°s r√°pidos dentro de los GRASP, mientras que *Grasp 0.75-100* es *el m√°s lento dentro de esta categor√≠a* con un tiempo de *2.32 segundos*. Esto sugiere que, aunque incrementar el n√∫mero de iteraciones puede mejorar ligeramente la calidad de la soluci√≥n, *tambi√©n aumenta el tiempo de ejecuci√≥n de manera considerable*.

        - Las ejecuciones del *algoritmo de b√∫squeda tab√∫ (TS1 y TS2)* tienen *tiempos de ejecuci√≥n moderados*, con *TS1* tomando *7.60 segundos* y *TS2 2.82 segundos*. *TS2* es notablemente m√°s r√°pido y tambi√©n produce soluciones de mejor calidad, lo que indica que *una configuraci√≥n de par√°metros m√°s ajustada* puede *mejorar tanto la eficiencia* como *la efectividad del algoritmo* de b√∫squeda tab√∫.

        - *El algoritmo de recocido simulado (SA1, SA2, SA3)* muestra tiempos m√°s altos, especialmente *SA1 y SA2*, que *tienen tiempos de 3.31 y 3.97 segundos* respectivamente.

        - La b√∫squeda local con soluci√≥n inicial heur√≠stica y aleatoria tienen tiempos similares, de 1.35 y 1.38 segundos respectivamente*. Sin embargo, la b√∫squeda local con soluci√≥n inicial heur√≠stica produce soluciones de mejor calidad, lo que sugiere que *el tiempo adicional invertido en una buena soluci√≥n inicial es beneficioso.*

        - *La b√∫squeda local iterativa (ILS1 y ILS2)* *muestra tiempos de 2.41 y 1.07 segundos* respectivamente, con *ILS2 siendo m√°s r√°pido*. Esto indica que configuraciones con menos iteraciones pueden ser m√°s eficientes sin sacrificar mucho en t√©rminos de calidad de soluci√≥n.

        - *El algoritmo gen√©tico es el m√°s lento*, con *GA3 siendo el m√°s lento con un tiempo de 141.46 segundos*. *Las otras configuraciones del algoritmo gen√©tico (GA1 y GA2) tambi√©n son lentas*, con *tiempos de 17.25 y 16.39 segundos respectivamente*, lo que los hace ineficientes para problemas donde el tiempo de ejecuci√≥n es cr√≠tico.

    - **Para n=25:**
        - *La heur√≠stica sigue siendo el algoritmo m√°s r√°pido con un tiempo de 0.00033 segundos.* *El algoritmo GRASP mantiene tiempos de ejecuci√≥n bajos*, con *Grasp 0.25-30* siendo el m√°s r√°pido con *0.31 segundos* y *Grasp 0.75-100 el m√°s lento con 1.24 segundos*. Esto confirma que las configuraciones de GRASP con menos iteraciones son m√°s r√°pidas y a√∫n efectivas, haciendo de GRASP una opci√≥n robusta para diferentes tama√±os de problema.

        - *El algoritmo de b√∫squeda tab√∫ (TS1 y TS2)* tiene *tiempos de ejecuci√≥n de 6.87 y 2.71 segundos respectivamente*. *La reducci√≥n en el tiempo de ejecuci√≥n de TS2 en comparaci√≥n con TS1 sigue siendo significativa*, reforzando la idea de que una configuraci√≥n optimizada del algoritmo de b√∫squeda tab√∫ puede ofrecer grandes mejoras en eficiencia.

        - *El algoritmo de recocido simulado (SA1, SA2, SA3) muestra tiempos moderados, con SA1 y SA2 en 3.33 y 3.96 segundos respectivamente*, mientras que *SA3* con 1,87 segundos, siendo la m√°s r√°pida que las otras dos configuraciones de reconocido simulado.

        - *La b√∫squeda local con soluci√≥n inicial heur√≠stica y aleatoria tienen tiempos de 1.22 segundos cada uno*, manteniendo consistencia en su eficiencia. La b√∫squeda local con soluci√≥n inicial heur√≠stica sigue siendo la mejor opci√≥n por su balance entre tiempo y calidad de la soluci√≥n.

        - *Las configuraciones de b√∫squeda local iterativa (ILS1 y ILS2) muestran tiempos de 2.28 y 0.91 segundos respectivamente*, con *ILS2 siendo nuevamente m√°s r√°pido*. Esto sugiere que ILS2 no solo es m√°s eficiente en t√©rminos de tiempo, sino que tambi√©n mantiene una buena calidad de soluci√≥n.

        - *El algoritmo gen√©ticos sigue siendo el m√°s lento*, con *GA3* *siendo el m√°s lento con un tiempo de 137.34 segundos*. *GA1 y GA2 tambi√©n presentan tiempos altos, con 16.70 y 20.28 segundos respectivamente.* Este alto costo computacional, junto con la menor calidad de soluci√≥n en comparaci√≥n con otros algoritmos, hace que el algoritmo gen√©tico con estas configuraciones sean menos atractivos para este tipo problemas de tama√±o n=25.
    
    **En resumen, GRASP con alpha = 0.25 y 100 iteraciones ofreci√≥ la mejor combinaci√≥n de precisi√≥n y tiempo de ejecuci√≥n, mientras que los algoritmos gen√©ticos fueron los menos eficientes en ambas m√©tricas.**
