# CI5652 - Identical Machine Scheduling
Proyecto de Dise√±o de Algoritmos II (CI5652) con soluciones aproximadas para el problema de Identical Machine Scheduling. Trimestre Abr-Jul 2024. Universidad Sim√≥n Bol√≠var.

## üìù Integrantes
- Ana Shek (19-10096)
- Santiago Finamore (18-10125)
- Jeamhowards Montiel (19-10234)

# ü§î Descripci√≥n del problema
Dado un conjunto de `n` tareas y `m` m√°quinas id√©nticas, el objetivo es asignar cada tarea a una m√°quina y determinar la secuencia de tareas en cada m√°quina de manera que se minimice la tardanza total (the total tardiness). Cada tarea `j` tiene un tiempo de procesamiento `p_j` y una fecha de vencimiento `d_j`. La tardanza de una tarea se calcula como `max(0, C_j - d_j)`, donde `C_j` es el tiempo de finalizaci√≥n del trabajo `j`.

# üìã INFORME DEL PROYECTO - SEGUNDO CORTE
El programa est√° implementado en C++ y consta de los siguientes archivos para este segundo corte:

- `grasp.cpp`: Archivo principal del programa que contiene la implementaci√≥n de una soluci√≥n utilizando GRASP para el problema.

üìÇ En la carpeta `benchmarks` se encuentran los casos de pruebas de la primera corte del proyecto para medir y comparar el rendimiento de diferentes algoritmos para solucionar el problema descrito.

## Definici√≥n de una perturbaci√≥n e implementaci√≥n de una b√∫squeda local iterada (ILS).

### üîÑ ILS

La implementaci√≥n del algoritmo de B√∫squeda Local Iterada (ILS) recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como el n√∫mero m√°ximo de iteraciones y la fuerza de perturbaci√≥n inicial. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Luego, aplica el algoritmo `RNA` a esta soluci√≥n inicial para obtener una soluci√≥n mejorada `S`.
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).

2. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se realiza una perturbaci√≥n a la soluci√≥n actual `current_schedule` aplicando un n√∫mero `p` de movimientos aleatorios.
    - Despu√©s de la perturbaci√≥n, se aplica el algoritmo `RNA` para mejorar la soluci√≥n perturbada.

3. **Evaluaci√≥n de soluciones:** 
    - Si la soluci√≥n mejorada tiene una tardanza total (`total_tardiness`) menor que la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n y se restablece la fuerza de perturbaci√≥n `p` a su valor inicial.
    - Si no mejora, se incrementa un contador `i`.

4. **Ajuste de la perturbaci√≥n:** 
    - Si el contador `i` alcanza el l√≠mite `itermax`, se incrementa la fuerza de perturbaci√≥n `p`. Si `p` excede un valor m√°ximo `pmax`, se restablece a su valor inicial.

5. **Repetici√≥n:** 
    - Se repiten los pasos 2-4 hasta que se agoten las iteraciones.

El algoritmo `ILS` busca explorar el espacio de soluciones mediante la combinaci√≥n de perturbaciones y optimizaci√≥n local, ayudando a escapar de √≥ptimos locales y encontrar soluciones mejores.

## Definici√≥n de reglas para movimientos que han de ser tab√∫s e implementaci√≥n de una b√∫squeda tab√∫.

### üö´ Tabu Search

La implementaci√≥n del algoritmo de B√∫squeda Tab√∫ recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como el n√∫mero m√°ximo de iteraciones y la tenencia de la lista tab√∫. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).

2. **Lista Tab√∫:** 
    - Se inicializa una lista tab√∫ para almacenar los movimientos que est√°n prohibidos temporalmente.

3. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se identifica la m√°quina con mayor tardanza (`tardiest_machine`).
    - Se generan m√∫ltiples vecinos de la soluci√≥n actual mediante movimientos aleatorios en la m√°quina con mayor tardanza.
    - Se selecciona el mejor vecino que no est√© en la lista tab√∫ o que mejora la mejor soluci√≥n conocida.

4. **Actualizaci√≥n de la lista Tab√∫:** 
    - Si el mejor vecino no est√° en la lista tab√∫ o mejora la mejor soluci√≥n conocida, se actualiza la soluci√≥n actual y se a√±ade el movimiento a la lista tab√∫.
    - Si la lista tab√∫ excede su tama√±o m√°ximo (`tabu_tenure`), se elimina el movimiento m√°s antiguo.

5. **Actualizaci√≥n de la mejor soluci√≥n:** 
    - Si el vecino seleccionado mejora la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n.

6. **Repetici√≥n:** 
    - Se repiten los pasos 3-5 hasta que se agoten las iteraciones.

El algoritmo `Tabu Search` busca explorar el espacio de soluciones evitando ciclos y escapando de √≥ptimos locales mediante el uso de una lista tab√∫ que proh√≠be ciertos movimientos temporalmente.

## Definici√≥n de un proceso de enfriado progresivo e implementaci√≥n de un recocido simulado.

### ‚ùÑÔ∏è Simulated Annealing

La implementaci√≥n del algoritmo de Recocido Simulado (Simulated Annealing) recibe la informaci√≥n de las `n` tareas, la cantidad de `m` m√°quinas, y varios par√°metros que controlan el proceso de b√∫squeda, como la temperatura inicial y el factor de reducci√≥n de temperatura. A continuaci√≥n se describen los pasos de la implementaci√≥n:

1. **Inicializaci√≥n:** 
    - El algoritmo empieza creando una soluci√≥n inicial `S` utilizando el m√©todo `mddScheduling`. 
    - Esta soluci√≥n `S` es considerada la mejor soluci√≥n conocida hasta el momento (`best_schedule`).
    - Se inicializa la temperatura `t` con un valor inicial `t0`.

2. **Iteraciones principales:** 
    - Para cada iteraci√≥n, se generan m√∫ltiples vecinos de la soluci√≥n actual mediante movimientos aleatorios.
    - Se calcula la diferencia de tardanza (`delta`) entre el vecino y la soluci√≥n actual.

3. **Criterio de aceptaci√≥n:** 
    - Si el vecino tiene una tardanza menor o igual, se acepta.
    - Si el vecino tiene una tardanza mayor, se acepta con una probabilidad `exp(-delta / t)`.

4. **Actualizaci√≥n de la mejor soluci√≥n:** 
    - Si la soluci√≥n actual mejorada tiene una tardanza menor que la mejor soluci√≥n conocida, se actualiza la mejor soluci√≥n.

5. **Reducci√≥n de la temperatura:** 
    - Despu√©s de un n√∫mero fijo de iteraciones, se reduce la temperatura multiplic√°ndola por un factor `t_step`.
    - Si la temperatura cae por debajo de un umbral `epsilon`, se restablece a su valor inicial `t0`.

6. **Repetici√≥n:** 
    - Se repiten los pasos 2-5 hasta que se agoten las iteraciones.

El algoritmo `Simulated Annealing` busca explorar el espacio de soluciones permitiendo peores soluciones con una probabilidad decreciente, lo que ayuda a escapar de √≥ptimos locales y encontrar mejores soluciones globales.

## üé≤ Definici√≥n de un m√©todo de construci√≥n para una RCL e implementaci√≥n de GRASP.

### üë®‚Äç‚öñÔ∏è M√©todo de construci√≥n para una RCL

Para la definici√≥n del RCL en este problema, se utiliz√≥ el enfoque heur√≠stico Modified Due Date (MDD) explicado en el primer corte del proyecto:

1. Sea `S` una soluci√≥n parcialmente construida.
2. Se tiene una lista de tareas no programadas `U` en la soluci√≥n `S`.
3. Para cada m√°quina `j`, dividir `U` en dos subconjuntos `U1j` y `U2j` para `j` = 1, 2, ..., `m`.
    > `U1j` contiene las tareas que no se pueden completar en su fecha de vencimiento en la m√°quina `j`.

    > `U2j` contiene las tareas que s√≠ se pueden completar antes de su fecha de vencimiento en la m√°quina `j`.
4. De `U1j` y `U2j`, encontrar los subconjuntos `Œ≥j` y `Œªj` que contienen las tareas con el tiempo de procesamiento m√≠nimo y la fecha de vencimiento m√≠nima, respectivamente. 
5. Seleccionar una tarea `gj` de `Œ≥j` o `Œªj` que minimice el valor de MDD en la m√°quina `j`. El valor de MDD en la m√°quina `j` de una tarea `i` est√° dada por `MDD(j, i) = max(Cj + pi, di)` 
    > `Cj` es la suma del procesamiento de tiempo de las tareas que ya han sido programados en la m√°quina `j`.

    > `pi` es el tiempo de procesamiento de la tarea `i` con su fecha de vencimiento `di`.
6. Luego, sea `C` el conjunto resultante de tener cada par `<g, l>`, donde cada una de estos pares representa que la tarea `g` es la tarea que produce el menor valor MDD en la m√°quina `l`).
7. Se puede definir el costo de la funci√≥n de un elemento `<g, l>` en `C` como `c(<g, l>) = MDD(g, l)`.
8. Tambi√©n se define `c_min = min{ c(<g, l>) | <g, l> ‚àà C}` y `c_max = max{ c(<g, l>) | <g, l> ‚àà C}`.
9. Entonces, el `RCL = { <g, l> ‚àà C | c(<g, l>) <= c_min + Œ±(c_max - c_min)}`

### üé∞ GRASP

La implementaci√≥n del algoritmo GRASP recibe la informaci√≥n de las `n` tareas, la cantidad de las `m` m√°quinas, el `alpha` para el RCL y `el m√°ximo n√∫mero de iteraciones`. A continuaci√≥n se describen los pasos de la implementaci√≥n: 

1. El algoritmo empieza a generar una soluci√≥n inicial aleatoria `S`. Por ahora, se tiene que `S` es la mejor soluci√≥n que se tiene para el problema.
2. Ahora, para cada iteraci√≥n, se construye una soluci√≥n voraz aleatoria `S'`, el cual: 

    2.1. Se considera las tareas que a√∫n no han sido programadas y se aplica el enfoque heur√≠stico para la Lista Restringida de Candidatos (RCL). 

    2.2. De esta lista RCL, se escoge aleatoriamente un elemento: `<g, l>`. 

    2.3. Luego, del elemento seleccionado `<g, l>`, se le asigna la tarea `g` a la m√°quina `l`. 

    2.4. Se elimina la tarea `g` de la lista de tareas sin programar en esta soluci√≥n parcial construida `S'`.

    2.5. Se repite los pasos 2.1 a 2.4 hasta que no queden tareas sin programar.
3. Se reemplaza `S` por `S'` si el retraso total o total tardiness de las tareas en la soluci√≥n `S` es mayor que el de `S'`, en caso contrario, no se hace nada.
4. Se repite el paso 2 y 3 hasta que se acaben las iteraciones.

## Definici√≥n de un fenotipo/genotipo, operadores de cruce y mutaci√≥n e implementaci√≥n de un algoritmo gen√©tico.

## üöÄ Uso

Para compilar y ejecutar el programa, se debe ejecutar los siguientes comandos en la terminal:

```bash
make
```

```bash
cd target
```

```bash
./PROY2 <path_to_benchmarks> <algorithm>
```

Donde `<path_to_benchmarks>` es la ruta a la carpeta que contiene los casos de prueba y `<algorithm>` es el n√∫mero del algoritmo a ejecutar:

1. B√∫squeda Local Iterada (ILS)
2. B√∫squeda Tab√∫ 
3. Reconocido Simulado
4. GRASP
5. Algoritmo Gen√©tico

## üìÑ Analisis de resultados

A continuaci√≥n se presenta el an√°lisis de los resultados obtenidos al ejecutar el programa con los casos de prueba en la carpeta `benchmarks` en una **laptop** con **procesador AMD Ryzen 5 5500U**, **disco SSD**, **8GB de memoria RAM** y **WSL 2 Ubuntu**. Se tomar√° en cuenta los valores de tardanza √≥ptima obtenidos del paper de referencia de donde se obtuvo el `benchmark` para comparar los resultados obtenidos con los algoritmos implementados.


### üìä M√©tricas Comparativas

Las m√©tricas claves en el an√°lisis incluyen:

- **Tardanza Total (Total Tardiness)**: La suma de las tardanzas de todas las tareas.
- **Diferencia con la Soluci√≥n √ìptima (Optimal Solution Difference)**: La diferencia entre la soluci√≥n obtenida y la soluci√≥n √≥ptima.

### üìà Resultados


## üìå Conclusiones



## üìö Referencias
